

### **Project 3: The Testing Showdown - Humans vs. LLMs in Bug Detection**

**1. Context & Motivation:**
At the **Software Engineering Research Centre (SERC), IIIT Hyderabad**, we need to choose the best strategy for testing critical code. Should we task an LLM to write tests, or should a human engineer do it? This project is a hands-on experiment to answer that question. Your team will first create a set of challenging, buggy functions. Your job is not to fix the bugs, but to determine **how best to find them.**

**2. Primary Objective:**
To run a head-to-head competition between two testing strategies and report which one is more effective at uncovering the subtle bugs you have created.

**3. Core Input:**
A selection of 25-30 correct functions from the **MBPP (Most Basic Programming Problems)** dataset.

**4. Core Output & Deliverables:**
Your team must produce a **"Bug Portfolio"** and a **clear, evidence-based project report** that answers the question: "Which method found more of the bugs we created?"

**A. The Bug Factory (Phase 1 - Creative Construction):**
*   For each correct MBPP function, your team must create **one buggy version**. Each version should contain a **single, minimal, and plausible bug**.
*   **Guidelines for Inserting Minimal Bugs:**
    *   **Think Subtly:** The goal is a bug that doesn't crash on simple inputs but produces wrong answers for specific cases.
    *   **Classic Mistakes:** Introduce off-by-one errors, edge case neglect (empty inputs, zero), incorrect conditionals (`>=` vs. `>`), wrong operator (`+` instead of `-`), or initialization errors.
    *   **Provide a Description:** For each function, write a clear, plain-English description of what it is *supposed* to do (this is what you will give to the LLM).

**B. The Testing Arena (Phase 2 - The Showdown):**
For each buggy function you created, you will run two testing strategies:
1.  **The LLM Challenger (Automated Testing):**
    *   **Task:** Feed the function signature and your description to an LLM. Prompt it to generate a comprehensive set of example-based unit tests (using `pytest`). Execute these tests against your buggy version.
    *   **Deliverable:** A log of the prompts used and the raw test code generated by the LLM.
2.  **The Human Defender (Strategic Testing):**
    *   **Task:** Your team will use the `hypothesis` library. Your goal is not to write examples, but to describe **general rules or properties** that the function must always obey.
    *   **Example:** For a sorting function, a property is: "The output list must be a permutation of the input list."
    *   **Deliverable:** The `hypothesis` code you write to check these properties.

**C. The Analysis Report (Phase 3 - The Verdict):**
This is the core of your grade. For each buggy function, your report must include:
1.  **The Bug Dossier:** A description of the bug you inserted.
2.  **Results:**
    *   Did the LLM's tests find the bug? (Yes/No)
    *   Did your Human-defined properties find the bug? (Yes/No)
3.  **Critical Commentary:** An analysis of *why* each method succeeded or failed.
4.  **Final Scorecard:** A summary table showing the results across all your functions:
    *   Bugs found only by LLM tests: [Number]
    *   Bugs found only by Human properties: [Number]
    *   Bugs found by both: [Number]
    *   Bugs found by neither: [Number]
5.  **Conclusion:** State which method won your showdown and explain **what types of bugs each method is good at catching.**


**5. References & Resources:**
* 	**Link:** [Google Research MBPP GitHub Repository](https://github.com/google-research/google-research/tree/master/mbpp)
    *   The dataset file is typically named `mbpp.jsonl` or `mbpp.json` in this repository.


**6. Technology Stack:**
*   **Language:** Python 3
*   **Testing:** `pytest`, `hypothesis`
*   **LLM:** Any model of your choice (e.g., DeepSeek, ChatGPT)
*   **Reporting:** Jupyter Notebook or PDF report.

**7. Team Size:** 5

**8. Point of Contact:** Abhishek Singh (abhishek.singh@iiit.ac.in)

